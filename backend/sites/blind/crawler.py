"""
ë¸”ë¼ì¸ë“œ(Blind) ì±„ìš© ê³µê³  í¬ë¡¤ëŸ¬
"""
import json
import re
import time
import urllib.parse
from pathlib import Path
from playwright.sync_api import sync_playwright, Page, Browser
from typing import List, Dict, Optional
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent))
from utils.logger import setup_logger
from utils.file_handler import save_json, create_job_data


class BlindCrawler:
    """ë¸”ë¼ì¸ë“œ ì±„ìš© ê³µê³  í¬ë¡¤ëŸ¬"""

    def __init__(self, headless: bool = True):
        """
        Args:
            headless: ë¸Œë¼ìš°ì €ë¥¼ í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ì‹¤í–‰í• ì§€ ì—¬ë¶€
        """
        self.logger = setup_logger("BlindCrawler")
        self.headless = headless
        self.config = self._load_config()
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None

    def _load_config(self) -> dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        config_path = Path(__file__).parent / "config.json"
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def start(self):
        """ë¸Œë¼ìš°ì € ì‹œì‘ - Bot Detection íšŒí”¼ ì ìš©"""
        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch(
            headless=self.headless,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--no-sandbox',
                '--disable-dev-shm-usage',
            ]
        )
        self.page = self.browser.new_page(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )
        self.page.set_viewport_size({"width": 1920, "height": 1080})
        self.page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)
        self.logger.info("ë¸Œë¼ìš°ì € ì‹œì‘ ì™„ë£Œ (Bot Detection íšŒí”¼ ì ìš©)")

    def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        if self.browser:
            self.browser.close()
        if hasattr(self, 'playwright'):
            self.playwright.stop()
        self.logger.info("ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")

    def _clean_text(self, text: str, max_length: int = None) -> str:
        """
        í…ìŠ¤íŠ¸ ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ê³µë°±, ê°œí–‰ ì œê±°)

        Args:
            text: ì •ë¦¬í•  í…ìŠ¤íŠ¸
            max_length: ìµœëŒ€ ê¸¸ì´ (Noneì´ë©´ ì œí•œ ì—†ìŒ)

        Returns:
            ì •ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        if not text:
            return ""

        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ, ê°œí–‰ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n+', '\n', text)
        text = text.strip()

        if max_length and len(text) > max_length:
            text = text[:max_length] + "..."

        return text

    def search(self, keyword: str) -> bool:
        """
        í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ - /jobs í˜ì´ì§€ì—ì„œ ê²€ìƒ‰ì°½ ì´ìš©

        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ

        Returns:
            ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # /jobs í˜ì´ì§€ë¡œ ì´ë™
            self.logger.info(f"ë¸”ë¼ì¸ë“œ Jobs í˜ì´ì§€ë¡œ ì´ë™")
            self.page.goto(self.config["jobs_url"], wait_until="domcontentloaded", timeout=60000)
            time.sleep(3)

            # ê²€ìƒ‰ì°½ ì°¾ê¸° ë° ì…ë ¥
            search_input = self.page.query_selector('input[placeholder*="Search by job title or company"], input[type="search"], input[aria-label*="Search"]')
            if search_input:
                search_input.fill(keyword)
                time.sleep(1)
                
                # Enter í‚¤ë¡œ ê²€ìƒ‰
                search_input.press("Enter")
                time.sleep(3)
                
                self.page.wait_for_load_state("networkidle", timeout=15000)
                self.logger.info(f"'{keyword}' ê²€ìƒ‰ ì™„ë£Œ")
                return True
            else:
                self.logger.warning("ê²€ìƒ‰ì°½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False

        except Exception as e:
            self.logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return False

    def get_job_list(self) -> List[str]:
        """
        í˜„ì¬ í˜ì´ì§€ì˜ ê³µê³  ëª©ë¡ì—ì„œ ê³µê³  ë§í¬ ìˆ˜ì§‘

        âš ï¸ í˜„ì¬ ì œí•œì‚¬í•­:
        - BlindëŠ” Bot Detectionìœ¼ë¡œ ìë™ í¬ë¡¤ë§ ì°¨ë‹¨
        - /job/search ì—”ë“œí¬ì¸íŠ¸: Bot Detection ì—ëŸ¬
        - /jobs ì—”ë“œí¬ì¸íŠ¸: ë¡œê·¸ì¸ í•„ìš”
        - í˜„ì¬ ë²„ì „ì—ì„œëŠ” ê³µê³  ìˆ˜ì§‘ ë¶ˆê°€ëŠ¥

        ìì„¸í•œ ë‚´ìš©ì€ IMPLEMENTATION_STATUS.md ì°¸ì¡°

        Returns:
            ê³µê³  ë§í¬ ë¦¬ìŠ¤íŠ¸ (í˜„ì¬ëŠ” í•­ìƒ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ì—ëŸ¬)
        """
        job_links = []
        try:
            # í˜ì´ì§€ ìƒíƒœ í™•ì¸
            body_text = self.page.evaluate("document.body.innerText")

            # Bot Detection ì—ëŸ¬ ì²´í¬
            if "Oops" in body_text or "Something went wrong" in body_text:
                self.logger.error("âŒ Bot Detection ê°ì§€ë¨ - BlindëŠ” ìë™ í¬ë¡¤ë§ì„ ì°¨ë‹¨í•©ë‹ˆë‹¤")
                self.logger.error("ì—ëŸ¬ ë©”ì‹œì§€: " + body_text[:200])
                self.logger.info("ğŸ’¡ í•´ê²° ë°©ë²•: IMPLEMENTATION_STATUS.md ì°¸ì¡°")
                return []

            # Login wall ì²´í¬
            if "Sign in" in body_text or "Log in" in body_text:
                self.logger.warning("âš ï¸ ë¡œê·¸ì¸ í•„ìš” - Blind ì±„ìš© ê³µê³ ëŠ” ë¡œê·¸ì¸ í›„ ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤")
                self.logger.info("ğŸ’¡ í˜„ì¬ ë²„ì „ì—ì„œëŠ” Blind í¬ë¡¤ë§ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                return []

            # í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            time.sleep(3)

            # ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ê³µê³  ë¡œë“œ
            for i in range(3):
                self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                time.sleep(2)

            # JavaScriptë¡œ ë§í¬ ìˆ˜ì§‘ - ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ
            links_data = self.page.evaluate("""
                () => {
                    const links = [];
                    const baseUrl = 'https://www.teamblind.com';
                    
                    // ë¸”ë¼ì¸ë“œ Jobs í˜ì´ì§€ì˜ ì‹¤ì œ êµ¬ì¡° ë¶„ì„
                    // ì´ë¯¸ì§€ ë§í¬ë‚˜ í´ë¦­ ê°€ëŠ¥í•œ ìš”ì†Œë“¤ì„ ì°¾ì•„ì„œ ë¶€ëª¨ ë§í¬ ì¶”ì¶œ
                    const jobCards = document.querySelectorAll('[class*="job"], [class*="Job"], img[alt*="company"], [data-testid*="job"]');
                    
                    jobCards.forEach(card => {
                        // ë¶€ëª¨ ìš”ì†Œì—ì„œ ë§í¬ ì°¾ê¸°
                        let parent = card;
                        for (let i = 0; i < 5; i++) {
                            if (parent && parent.tagName === 'A' && parent.href) {
                                const href = parent.href;
                                if (href.includes('/jobs/') || href.includes('/job/')) {
                                    if (!links.includes(href) && !href.includes('/jobs?') && !href.includes('/job/search')) {
                                        links.push(href);
                                    }
                                    break;
                                }
                            }
                            parent = parent.parentElement;
                            if (!parent) break;
                        }
                    });
                    
                    // ì¼ë°˜ ë§í¬ì—ì„œë„ ì°¾ê¸°
                    document.querySelectorAll('a[href]').forEach(link => {
                        const href = link.getAttribute('href');
                        if (href && (href.includes('/jobs/') || href.includes('/job/'))) {
                            if (!href.includes('/jobs?') && !href.includes('/job/search') && !href.match(/\\/jobs?\\/?(\\?|#|$)/)) {
                                let fullUrl = href.startsWith('http') ? href : baseUrl + href;
                                if (!links.includes(fullUrl)) {
                                    links.push(fullUrl);
                                }
                            }
                        }
                    });
                    
                    return [...new Set(links)];
                }
            """)

            job_links = links_data if links_data else []

            if len(job_links) == 0:
                self.logger.warning("âš ï¸ ê³µê³  ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                self.logger.info("ğŸ’¡ BlindëŠ” Bot Detection ë˜ëŠ” Login wallë¡œ í¬ë¡¤ë§ì´ ì œí•œë©ë‹ˆë‹¤")
                self.logger.info("ğŸ“– ìì„¸í•œ ë‚´ìš©: sites/blind/IMPLEMENTATION_STATUS.md")
            else:
                self.logger.info(f"ì´ {len(job_links)}ê°œì˜ ê³µê³  ë§í¬ ìˆ˜ì§‘")
                self.logger.debug(f"ìƒ˜í”Œ ë§í¬: {job_links[:3]}")

            return job_links

        except Exception as e:
            self.logger.error(f"ê³µê³  ëª©ë¡ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            self.logger.info("ğŸ’¡ Blind í¬ë¡¤ë§ ì œí•œì‚¬í•­: sites/blind/IMPLEMENTATION_STATUS.md")
            return []

    def parse_job_detail(self, job_url: str) -> Optional[Dict]:
        """
        ê³µê³  ìƒì„¸ í˜ì´ì§€ì—ì„œ ì •ë³´ íŒŒì‹±

        Args:
            job_url: ê³µê³  ìƒì„¸ í˜ì´ì§€ URL

        Returns:
            íŒŒì‹±ëœ ê³µê³  ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            self.logger.debug(f"ê³µê³  ìƒì„¸ í˜ì´ì§€ ì ‘ì†: {job_url}")
            self.page.goto(job_url, wait_until="domcontentloaded", timeout=60000)
            time.sleep(self.config.get("wait_time", 5))

            # ë„¤íŠ¸ì›Œí¬ ì•ˆì •í™” ëŒ€ê¸°
            try:
                self.page.wait_for_load_state("networkidle", timeout=10000)
            except:
                pass

            job_info = {
                "url": job_url,
                "title": "",
                "company": "",
                "location": "",
                "salary": "",
                "conditions": "",
                "detail": "",
                "recruit_summary": "",
                "posted_date": ""
            }

            # JavaScriptë¡œ ì •ë³´ ì¶”ì¶œ
            parsed_data = self.page.evaluate("""
                () => {
                    const result = {
                        title: '',
                        company: '',
                        location: '',
                        salary: '',
                        conditions: '',
                        detail: '',
                        recruit_summary: '',
                        posted_date: ''
                    };

                    // ì œëª© ì¶”ì¶œ (h1)
                    const h1 = document.querySelector('h1');
                    if (h1) {
                        result.title = h1.innerText.trim();
                    }

                    // ì „ì²´ í…ìŠ¤íŠ¸
                    const bodyText = document.body.innerText;
                    const lines = bodyText.split('\\n').map(l => l.trim()).filter(l => l);

                    // íšŒì‚¬ëª… (í‰ì  ì•ì— ìˆìŒ)
                    for (let i = 0; i < lines.length; i++) {
                        if (lines[i].match(/^[0-9]\\.[0-9]$/)) {
                            if (i > 0) {
                                result.company = lines[i - 1];
                            }
                        }
                    }

                    // ìƒì„¸ ë‚´ìš©
                    const main = document.querySelector('main, article, [role="main"]');
                    result.detail = main ? main.innerText.trim() : bodyText;

                    // ê·¼ë¬´ì§€ íŒ¨í„´
                    const locMatch = result.detail.match(/(?:Remote|Hybrid|On-site|USA|United States)[^\\n]{0,100}/i);
                    if (locMatch) {
                        result.location = locMatch[0];
                    }

                    // ê¸‰ì—¬ íŒ¨í„´
                    const salMatch = result.detail.match(/\\$[\\d,]+[^\\n]{0,100}/);
                    if (salMatch) {
                        result.salary = salMatch[0];
                    }

                    // ìê²©ìš”ê±´
                    const condMatch = result.detail.match(/(?:Requirements?|Qualifications?|Skills?)[\\s\\S]{0,800}/i);
                    if (condMatch) {
                        result.conditions = condMatch[0];
                    }

                    // ëª¨ì§‘ìš”ê°•
                    const recruitMatch = result.detail.match(/(?:About|Description|Responsibilities)[\\s\\S]{0,1500}/i);
                    if (recruitMatch) {
                        result.recruit_summary = recruitMatch[0];
                    } else {
                        result.recruit_summary = result.detail.substring(0, 1000);
                    }

                    return result;
                }
            """)

            # íŒŒì‹±ëœ ë°ì´í„° ë°˜ì˜
            job_info.update(parsed_data)

            # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¬ì¶”ì¶œ (fallback)
            if job_info["detail"]:
                job_info = self._extract_fields_from_detail(job_info)

            # í…ìŠ¤íŠ¸ ì •ë¦¬
            job_info["location"] = self._clean_text(job_info["location"], max_length=200)
            job_info["salary"] = self._clean_text(job_info["salary"], max_length=100)
            job_info["conditions"] = self._clean_text(job_info["conditions"], max_length=500)
            job_info["recruit_summary"] = self._clean_text(job_info["recruit_summary"], max_length=2000)
            job_info["posted_date"] = self._clean_text(job_info["posted_date"], max_length=50)

            # ì œëª©ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if not job_info["title"]:
                self.logger.warning(f"ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìŠ¤í‚µ: {job_url}")
                return None

            self.logger.info(f"ê³µê³  íŒŒì‹± ì™„ë£Œ: {job_info['title']} - {job_info.get('company', 'N/A')}")
            return job_info

        except Exception as e:
            self.logger.error(f"ê³µê³  ìƒì„¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ({job_url}): {e}", exc_info=True)
            return None

    def _extract_fields_from_detail(self, job_info: Dict) -> Dict:
        """
        detail í•„ë“œì—ì„œ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ í•„ë“œ ì¬ì¶”ì¶œ (fallback)

        Args:
            job_info: ê³µê³  ì •ë³´ ë”•ì…”ë„ˆë¦¬

        Returns:
            ì—…ë°ì´íŠ¸ëœ ê³µê³  ì •ë³´
        """
        detail = job_info.get("detail", "")
        if not detail:
            return job_info

        # ê¸‰ì—¬ ì¶”ì¶œ
        if not job_info.get("salary"):
            salary_patterns = [
                r'\$[\d,]+[\s\-~]*(?:to|-)?\s*\$?[\d,]+',
                r'[\d,]+K[\s\-~]*(?:to|-)?\s*[\d,]+K',
            ]
            for pattern in salary_patterns:
                match = re.search(pattern, detail, re.IGNORECASE)
                if match:
                    job_info["salary"] = match.group(0).strip()
                    break

        # ê·¼ë¬´ì§€ ì¶”ì¶œ
        if not job_info.get("location"):
            location_patterns = [
                r'(?:Remote|Hybrid|On-site)[^\n]{0,100}',
                r'(?:USA|United States)[^\n]{0,100}',
            ]
            for pattern in location_patterns:
                match = re.search(pattern, detail, re.IGNORECASE)
                if match:
                    job_info["location"] = match.group(0).strip()
                    break

        return job_info

    def crawl(self, keyword: str, max_jobs: int = 50) -> List[Dict]:
        """
        í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•˜ì—¬ ê³µê³  ìˆ˜ì§‘

        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_jobs: ìµœëŒ€ ìˆ˜ì§‘í•  ê³µê³  ìˆ˜

        Returns:
            ìˆ˜ì§‘ëœ ê³µê³  ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        self.logger.info(f"'{keyword}' í‚¤ì›Œë“œë¡œ í¬ë¡¤ë§ ì‹œì‘")

        if not self.search(keyword):
            return []

        all_jobs = []
        job_links = self.get_job_list()

        if not job_links:
            self.logger.warning("ê³µê³  ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return []

        # ìµœëŒ€ ê°œìˆ˜ë§Œí¼ë§Œ ìˆ˜ì§‘
        job_links = job_links[:max_jobs]

        for i, job_url in enumerate(job_links, 1):
            try:
                self.logger.info(f"ì§„í–‰ ì¤‘: {i}/{len(job_links)}")
                job_info = self.parse_job_detail(job_url)
                if job_info:
                    all_jobs.append(job_info)
                time.sleep(self.config.get("request_delay", 2))
            except Exception as e:
                self.logger.error(f"ê³µê³  ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({job_url}): {e}")
                continue

        self.logger.info(f"ì´ {len(all_jobs)}ê°œì˜ ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ")
        return all_jobs

    def save_results(self, keyword: str, jobs: List[Dict]):
        """
        ìˆ˜ì§‘ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥

        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            jobs: ìˆ˜ì§‘ëœ ê³µê³  ë¦¬ìŠ¤íŠ¸
        """
        data = create_job_data(
            site=self.config["site_name"],
            keyword=keyword,
            jobs=jobs
        )

        # íŒŒì¼ëª… ìƒì„±
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_keyword = re.sub(r'[<>:"/\\|?*]', '_', keyword)
        filename = f"blind_{safe_keyword}_{timestamp}.json"

        filepath = save_json(data, filename)
        self.logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    crawler = BlindCrawler(headless=False)
    try:
        crawler.start()
        # "semiconductor" í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ (ë¸”ë¼ì¸ë“œëŠ” ì˜ë¬¸ ì‚¬ì´íŠ¸)
        jobs = crawler.crawl("semiconductor", max_jobs=3)
        if jobs:
            crawler.save_results("semiconductor", jobs)
            print(f"\nìˆ˜ì§‘ëœ ê³µê³  ìˆ˜: {len(jobs)}")
            for job in jobs:
                print(f"- {job['title']} at {job.get('company', 'N/A')}")
    finally:
        crawler.close()
