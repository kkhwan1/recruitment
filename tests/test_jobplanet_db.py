"""
ì¡í”Œë˜ë‹› í¬ë¡¤ëŸ¬ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ í…ŒìŠ¤íŠ¸
"""
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent))

from sites.jobplanet.crawler import JobplanetCrawler
from database.repositories import JobRepository
from database.connection import get_db_connection
from datetime import datetime

def test_crawl_and_save():
    """í¬ë¡¤ë§ í›„ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ í…ŒìŠ¤íŠ¸"""
    print("=" * 80)
    print("ì¡í”Œë˜ë‹› í¬ë¡¤ëŸ¬ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ í…ŒìŠ¤íŠ¸")
    print("=" * 80)

    crawler = JobplanetCrawler(headless=False)

    try:
        # 1. í¬ë¡¤ë§
        print("\n[1ë‹¨ê³„] í¬ë¡¤ë§ ì‹œì‘...")
        crawler.start()
        keyword = "ë°˜ë„ì²´"
        jobs = crawler.crawl(keyword, max_jobs=3)

        print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(jobs)}ê°œì˜ ê³µê³ ")

        if not jobs:
            print("âŒ ìˆ˜ì§‘ëœ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 2. JSON ì €ì¥
        print("\n[2ë‹¨ê³„] JSON íŒŒì¼ ì €ì¥...")
        crawler.save_results(keyword, jobs)

        # 3. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        print("\n[3ë‹¨ê³„] ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥...")
        repo = JobRepository()

        saved_count = 0
        error_count = 0

        for i, job in enumerate(jobs, 1):
            try:
                # ì‹œê°„ ì •ë³´ ì¶”ê°€
                now = datetime.now()
                job['source_site'] = "ì¡í”Œë˜ë‹›"
                job['search_keyword'] = keyword
                job['crawled_at'] = now

                # ì €ì¥ (insert_job ë©”ì„œë“œëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°›ìŒ)
                job_id = repo.insert_job(job)
                print(f"  âœ“ [{i}/{len(jobs)}] ì €ì¥ ì„±ê³µ (ID: {job_id}): {job.get('title', '')[:50]}")
                saved_count += 1

            except Exception as e:
                print(f"  âœ— [{i}/{len(jobs)}] ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                error_count += 1
                import traceback
                traceback.print_exc()

        # 4. ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 80)
        print("ìµœì¢… ê²°ê³¼")
        print("=" * 80)
        print(f"ğŸ“Š ìˆ˜ì§‘ëœ ê³µê³ : {len(jobs)}ê°œ")
        print(f"âœ… ì €ì¥ ì„±ê³µ: {saved_count}ê°œ")
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {error_count}ê°œ")

        if saved_count > 0:
            print(f"\nâœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        else:
            print(f"\nâŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨!")

    finally:
        crawler.close()


if __name__ == "__main__":
    test_crawl_and_save()
